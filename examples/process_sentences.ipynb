{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c6c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This loads the variables from .env\n",
    "api_key = os.getenv('OPENAI_API_KEY')  # This gets a specific variable\n",
    "\n",
    "# Add the src directory to Python path\n",
    "src_path = \"src\"\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "from matsci_llm_causality.extraction.pdf import PDFProcessor\n",
    "from matsci_llm_causality.models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcf38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "pdf_processor = PDFProcessor()\n",
    "entity_recognizer = create_model(\"gpt-5-entity\")\n",
    "relation_extractor = create_model(\"gpt-5-relation\")\n",
    "\n",
    "# Path to your PDF\n",
    "pdf_path = Path(\"D:/Research/LLM4Causal/tests/data/sciadv.abo6043.pdf\")  # Replace with your PDF path\n",
    "text_path = Path(\"tests/data/text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d48805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Modern genomics combined with advanced bioinformatics methodologies allow us to understand much more about complex living systems than was ever previously possible. In the realm of human biology, for instance, recent developments have given us the ability to pinpoint the genes influencing diseases such as cancers. One area where these novel technologies can be anticipated to exert a huge impact but have thus far remained underused is the study of structural biomaterials. Spider silk is a prime example of an extended phenotype, whose extraordinary mechanical properties are governed by the underlying composition and structure of protein building blocks called spidroins.', 'All spiders use silk for various critical purposes, including foraging, locomotion, nesting, mating, egg protection, and communication  (1) . Different types of threads are used for diverse purposes, each produced in specific glands in the abdomen  (2) . For example, orb-weaving spiders use up to seven different types of silks, named after the gland that produces these threads. Major ampullate silk is the toughest silk used as draglines and as frames of orb webs, minor ampullate silk is used as scaffold during orb web weaving, piriform silk adheres the frame of the orb web to wood or other substrates, and capture thread of the orb web is composed of flagelliform silk backbone and aggregate glue. Aciniform silk is used for prey wrapping and sometimes for decorations of the web, and tubiform (or cylindrical) silk is used to make an egg sac. While spiders are successful predators and are often associated with orb webs, orb-weaving spiders of superfamily Araneoidea only comprise about 25% of spider species. A more ancestral clade of spiders such as those belonging to the infraorder Mygalomorphae is comprised mostly of ground-wandering spiders that produce sheet and maze webs for prey capture. Wandering hunters and abandoned silk capture webs make up a more modern clade of spiders in the retrolateral tibial apophysis (RTA) clade; this group comprises as much as 50% of all spider species  (3) . Therefore, spiders have diversified, selected, and specialized various uses of silk adapting to their ecological needs. Such extraordinary plasticity and university of silk and silk proteins is an ideal target to model the link between sequence and its physical property to fully understand the underlying design principles to apply the wide range of physical properties as biomaterials.', 'Spider silks are renowned for their diverse and impressive mechanical properties, frequently displaying a combination of high tensile strength, extensibility, and exceptional toughness that is unmatched industrially. Hence, the processing-property space that these silks occupy makes them a unique source of inspiration for protein biopolymer materials with low embodied energy and high performance  (4) (5) (6) . However, this property space has yet to be fully explored, defined, and exploited. Silk fiber diversity scales rapidly, as spiders produce multiple types of silk, each of which are composed of specific proteins known as spidroins, whose mostly monophyletic origins  (7)  endow them with specific mechanical properties  (2, 8) . One type of spider silk protein, major ampullate spidroin (MaSp; which is often included in dragline threads), has received substantial academic and industrial attention, as this silk typically shows strength and toughness comparable to those of synthetic high-performance fibers, with an approximately 1-GPa breaking strength, a 30% breaking strain, and a toughness of 130 to 200 MJ/m 3  (9) (10) (11) . However, there are lesser-known taxa and species of spiders, suggesting that the limits of silk properties are yet to be defined  (12) . On the other hand, a unique property known as supercontraction, where the dragline silk shrinks in length by up to 60% when wetted, is often considered undesirable industrially, and expectations are high for protein engineering methods to reduce such property by modifying the primary sequence. Hence, a comprehensive, coordinated global effort combining taxonomy, genomics, and materiomics is required to first understand and then unlock the true potential of these materials  (13) .']\n"
     ]
    }
   ],
   "source": [
    "with open(text_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "print(paragraphs[:3])  # show first 3 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9126d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase 1] Extracting entities from 43 paragraphs...\n",
      "  - Processed 10/43 paragraphs for entities\n",
      "  - Processed 10/43 paragraphs for entities\n",
      "  - Processed 20/43 paragraphs for entities\n",
      "  - Processed 20/43 paragraphs for entities\n",
      "  - Processed 30/43 paragraphs for entities\n",
      "  - Processed 30/43 paragraphs for entities\n",
      "  - Processed 40/43 paragraphs for entities\n",
      "  - Processed 40/43 paragraphs for entities\n",
      "  - Processed 43/43 paragraphs for entities\n",
      "[Phase 1 Done] Total unique entities: 287\n",
      "  - Processed 43/43 paragraphs for entities\n",
      "[Phase 1 Done] Total unique entities: 287\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MAX_WORKERS = 50  # tune to your rate limits\n",
    "\n",
    "def extract_entities_sync(paragraph):\n",
    "    # Sync call\n",
    "    return entity_recognizer.extract_entities(paragraph)\n",
    "\n",
    "def extract_relations_sync(paragraph, all_entities):\n",
    "    # Sync call\n",
    "    return relation_extractor.extract_relations(paragraph, all_entities)\n",
    "\n",
    "\n",
    "total = len(paragraphs)\n",
    "\n",
    "# ---------- Phase 1: Entities ----------\n",
    "print(f\"[Phase 1] Extracting entities from {total} paragraphs...\")\n",
    "all_entities = []\n",
    "seen = set()\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {ex.submit(extract_entities_sync, p): i for i, p in enumerate(paragraphs)}\n",
    "    for idx, fut in enumerate(as_completed(futures), 1):\n",
    "        try:\n",
    "            ents = fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error in entity extraction: {e}\")\n",
    "            continue\n",
    "\n",
    "        for ent in ents:\n",
    "            key = ent.text.lower()\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                all_entities.append(ent)\n",
    "\n",
    "        if idx % 10 == 0 or idx == total:  # print every 10 completions\n",
    "            print(f\"  - Processed {idx}/{total} paragraphs for entities\")\n",
    "\n",
    "print(f\"[Phase 1 Done] Total unique entities: {len(all_entities)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0d98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase 2] Extracting relations from 43 paragraphs...\n",
      "  - Processed 10/43 paragraphs for relations\n",
      "  - Processed 10/43 paragraphs for relations\n",
      "  - Processed 20/43 paragraphs for relations\n",
      "  - Processed 20/43 paragraphs for relations\n",
      "  - Processed 30/43 paragraphs for relations\n",
      "  - Processed 30/43 paragraphs for relations\n",
      "⚠️ Error in relation extraction: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-pcx0SC2kFfkxTUuztFPkUdSR on tokens per min (TPM): Limit 30000, Used 30000, Requested 4292. Please try again in 8.584s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ Error in relation extraction: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-pcx0SC2kFfkxTUuztFPkUdSR on tokens per min (TPM): Limit 30000, Used 30000, Requested 4261. Please try again in 8.522s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ Error in relation extraction: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-pcx0SC2kFfkxTUuztFPkUdSR on tokens per min (TPM): Limit 30000, Used 30000, Requested 4292. Please try again in 8.584s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ Error in relation extraction: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-pcx0SC2kFfkxTUuztFPkUdSR on tokens per min (TPM): Limit 30000, Used 30000, Requested 4261. Please try again in 8.522s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ Error in relation extraction: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-pcx0SC2kFfkxTUuztFPkUdSR on tokens per min (TPM): Limit 30000, Used 30000, Requested 4436. Please try again in 8.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ Error in relation extraction: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-pcx0SC2kFfkxTUuztFPkUdSR on tokens per min (TPM): Limit 30000, Used 30000, Requested 4436. Please try again in 8.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "⚠️ Error in relation extraction: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-pcx0SC2kFfkxTUuztFPkUdSR on tokens per min (TPM): Limit 30000, Used 30000, Requested 5113. Please try again in 10.226s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "[Phase 2 Done] Total unique relationships: 46\n",
      "⚠️ Error in relation extraction: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-pcx0SC2kFfkxTUuztFPkUdSR on tokens per min (TPM): Limit 30000, Used 30000, Requested 5113. Please try again in 10.226s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "[Phase 2 Done] Total unique relationships: 46\n"
     ]
    }
   ],
   "source": [
    "total = len(paragraphs)\n",
    "# ---------- Phase 2: Relations ----------\n",
    "print(f\"[Phase 2] Extracting relations from {total} paragraphs...\")\n",
    "rel_set = set()\n",
    "all_relationships = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {ex.submit(extract_relations_sync, p, all_entities): i for i, p in enumerate(paragraphs)}\n",
    "    for idx, fut in enumerate(as_completed(futures), 1):\n",
    "        try:\n",
    "            rels = fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error in relation extraction: {e}\")\n",
    "            continue\n",
    "\n",
    "        for rel in rels:\n",
    "            key = (rel.subject.text, rel.object.text, rel.relation_type)\n",
    "            if key not in rel_set:\n",
    "                rel_set.add(key)\n",
    "                all_relationships.append(rel)\n",
    "\n",
    "        if idx % 10 == 0 or idx == total:\n",
    "            print(f\"  - Processed {idx}/{total} paragraphs for relations\")\n",
    "\n",
    "print(f\"[Phase 2 Done] Total unique relationships: {len(all_relationships)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae83d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize empty lists to store unique entities and relationships\n",
    "# all_entities = []\n",
    "# all_relationships = []\n",
    "# entity_texts = set()  # To track unique entity texts\n",
    "\n",
    "# print(\"Processing paragraphs for entities and relationships...\")\n",
    "# for i, paragraph in enumerate(paragraphs, 1):\n",
    "#     print(f\"\\nProcessing paragraph {i}/{len(paragraphs)}...\")\n",
    "    \n",
    "#     # Extract entities from current paragraph\n",
    "#     new_entities = entity_recognizer.extract_entities(paragraph)\n",
    "    \n",
    "#     # Add only new entities\n",
    "#     entities_added = False\n",
    "#     for entity in new_entities:\n",
    "#         if entity.text.lower() not in entity_texts:\n",
    "#             entity_texts.add(entity.text.lower())\n",
    "#             all_entities.append(entity)\n",
    "#             entities_added = True\n",
    "    \n",
    "#     # Extract relationships using all known entities\n",
    "#     new_relationships = relation_extractor.extract_relations(paragraph, all_entities)\n",
    "    \n",
    "#     # Add new relationships (checking for duplicates based on subject, object, and type)\n",
    "#     relationships_added = False\n",
    "#     for rel in new_relationships:\n",
    "#         # Create a unique key for the relationship\n",
    "#         rel_key = (rel.subject.text, rel.object.text, rel.relation_type)\n",
    "#         if not any(\n",
    "#             (r.subject.text, r.object.text, r.relation_type) == rel_key \n",
    "#             for r in all_relationships\n",
    "#         ):\n",
    "#             all_relationships.append(rel)\n",
    "#             relationships_added = True\n",
    "    \n",
    "#     # If there were updates, print the current state\n",
    "#     if entities_added or relationships_added:\n",
    "#         print(\"\\nCurrent Entities:\")\n",
    "#         for entity in all_entities:\n",
    "#             print(f\"- {entity.text} ({entity.type.value})\")\n",
    "        \n",
    "#         print(\"\\nCurrent Relationships:\")\n",
    "#         if all_relationships:\n",
    "#             for rel in all_relationships:\n",
    "#                 print(f\"- {rel}\")\n",
    "#         else:\n",
    "#             print(\"No relationships found yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6d1327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "Total unique entities: 287\n",
      "Total unique relationships: 46\n",
      "\n",
      "Files saved successfully with complete data structures!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Total unique entities: {len(all_entities)}\")\n",
    "print(f\"Total unique relationships: {len(all_relationships)}\")\n",
    "\n",
    "# Create complete dictionaries for entities and relationships\n",
    "entities_dict = [\n",
    "    {\n",
    "        \"id\": entity.id,\n",
    "        \"text\": entity.text,\n",
    "        \"type\": entity.type.value,\n",
    "        \"aliases\": entity.aliases,\n",
    "        \"metadata\": entity.metadata\n",
    "    } \n",
    "    for entity in all_entities\n",
    "]\n",
    "\n",
    "relationships_dict = [\n",
    "    {\n",
    "        \"subject\": {\n",
    "            \"id\": rel.subject.id,\n",
    "            \"text\": rel.subject.text,\n",
    "            \"type\": rel.subject.type.value\n",
    "        },\n",
    "        \"object\": {\n",
    "            \"id\": rel.object.id,\n",
    "            \"text\": rel.object.text,\n",
    "            \"type\": rel.object.type.value\n",
    "        },\n",
    "        \"relation_type\": rel.relation_type.value,\n",
    "        \"polarity\": rel.polarity,\n",
    "        \"confidence\": rel.confidence,\n",
    "        \"evidence\": rel.evidence,\n",
    "        \"metadata\": rel.metadata\n",
    "    }\n",
    "    for rel in all_relationships\n",
    "] if all_relationships else []\n",
    "\n",
    "# Save entities to JSON file with complete information\n",
    "with open('entities.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(entities_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save relationships to JSON file with complete information\n",
    "with open('relationships.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(relationships_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nFiles saved successfully with complete data structures!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
