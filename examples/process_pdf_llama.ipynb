{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d657c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad45217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()  # This loads the variables from .env\n",
    "# api_key = os.getenv('OPENAI_API_KEY')  # This gets a specific variable\n",
    "\n",
    "# Add the src directory to Python path\n",
    "src_path = \"src\"\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "from matsci_llm_causality.extraction.pdf import PDFProcessor\n",
    "from matsci_llm_causality.models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfab46e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown model type: gpt-5-nano-2025-08-07",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m pdf_processor = PDFProcessor()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# entity_recognizer = create_model(\"scibert\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m relation_extractor = \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-5-nano-2025-08-07\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Path to your PDF\u001b[39;00m\n\u001b[32m      7\u001b[39m pdf_path = Path(\u001b[33m\"\u001b[39m\u001b[33mD:/Research/LLM4Causal/tests/data/sciadv.abo6043.pdf\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Replace with your PDF path\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/LLMHackathon/LLM4Causal/src/matsci_llm_causality/models/base.py:46\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(model_type, config)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create an instance of the specified model.\"\"\"\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _models:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     49\u001b[39m     config = ModelConfig(\n\u001b[32m     50\u001b[39m         model_type = model_type,\n\u001b[32m     51\u001b[39m         temperature=\u001b[32m1.0\u001b[39m,\n\u001b[32m     52\u001b[39m         max_length=\u001b[32m512\u001b[39m,\n\u001b[32m     53\u001b[39m         device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Unknown model type: gpt-5-nano-2025-08-07"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "pdf_processor = PDFProcessor()\n",
    "# entity_recognizer = create_model(\"scibert\")\n",
    "relation_extractor = create_model(\"gpt-5-nano-2025-08-07\")\n",
    "\n",
    "# Path to your PDF\n",
    "pdf_path = Path(\"D:/Research/LLM4Causal/tests/data/sciadv.abo6043.pdf\")  # Replace with your PDF path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract text from PDF\n",
    "print(\"Extracting text from PDF...\")\n",
    "text = pdf_processor.extract_text(pdf_path)\n",
    "print(f\"Extracted {len(text)} characters\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract entities using SciBERT\n",
    "print(\"Extracting entities...\")\n",
    "entities = entity_recognizer.extract_entities(text)\n",
    "print(\"\\nFound entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"- {entity.text} ({entity.type.value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59244602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract relationships using FLAN-T5\n",
    "print(\"\\nExtracting relationships...\")\n",
    "result = relation_extractor.extract_relations(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Print results\n",
    "print(\"\\nExtracted relationships:\")\n",
    "if result.relationships:\n",
    "    for rel in result.relationships:\n",
    "        print(f\"- {rel}\")\n",
    "else:\n",
    "    print(\"Raw FLAN-T5 response:\")\n",
    "    print(result.metadata[\"raw_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab60430",
   "metadata": {},
   "source": [
    "# Test Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from src.matsci_llm_causality.models.scibert import SciBERTEntityRecognizer\n",
    "from src.matsci_llm_causality.schema import EntityType\n",
    "\n",
    "# Test samples with known entities\n",
    "TEST_SAMPLES = [\n",
    "    {\n",
    "        \"text\": \"Silk fibroin exhibits increased crystallinity at higher temperatures.\",\n",
    "        \"entities\": [\n",
    "            {\"text\": \"Silk fibroin\", \"type\": EntityType.MATERIAL},\n",
    "            {\"text\": \"crystallinity\", \"type\": EntityType.PROPERTY},\n",
    "            {\"text\": \"temperatures\", \"type\": EntityType.CONDITION}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Beta-sheet content affects the mechanical properties through hydrogen bonding.\",\n",
    "        \"entities\": [\n",
    "            {\"text\": \"Beta-sheet content\", \"type\": EntityType.STRUCTURE},\n",
    "            {\"text\": \"mechanical properties\", \"type\": EntityType.PROPERTY}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_recognizer = GPT5EntityRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in TEST_SAMPLES:\n",
    "    entities = entity_recognizer.extract_entities(sample[\"text\"])\n",
    "    print(f\"Extract Entities: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68904d",
   "metadata": {},
   "source": [
    "# Process PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00bbad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jacob/anaconda3/envs/llm4mat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load_dotenv()  # This loads the variables from .env\n",
    "# api_key = os.getenv('OPENAI_API_KEY')  # This gets a specific variable\n",
    "\n",
    "# Add the src directory to Python path\n",
    "src_path = \"src\"\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "from matsci_llm_causality.extraction.pdf import PDFProcessor\n",
    "from matsci_llm_causality.models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b74bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Hugging Face\n",
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0950f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jacob/anaconda3/envs/llm4mat/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/Jacob/anaconda3/envs/llm4mat/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [01:36<01:33, 46.91s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "pdf_processor = PDFProcessor()\n",
    "# entity_recognizer = create_model(\"gpt-5-entity\")\n",
    "model=\"gpt-5-relation\"\n",
    "model=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "relation_extractor = create_model(model)\n",
    "\n",
    "# Path to your PDF\n",
    "pdf_path = Path(\"D:/Research/LLM4Causal/tests/data/sciadv.abo6043.pdf\")  # Replace with your PDF path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract text from PDF\n",
    "print(\"Extracting text from PDF...\")\n",
    "text = pdf_processor.extract_text(pdf_path)\n",
    "print(f\"Extracted {len(text)} characters\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Extract entities using SciBERT\n",
    "# print(\"Extracting entities...\")\n",
    "# entities = entity_recognizer.extract_entities(text)\n",
    "# print(\"\\nFound entities:\")\n",
    "# for entity in entities:\n",
    "#     print(f\"- {entity.text} ({entity.type.value})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f99953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Extract relationships using FLAN-T5\n",
    "print(\"\\nExtracting relationships...\")\n",
    "relationships = relation_extractor.extract_relations(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510372e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "def parse_relationships(text: str):\n",
    "    # Split text into lines, stripping empty ones\n",
    "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for line in lines:\n",
    "        # Regex to capture: [SubjectName][Type] relationship [ObjectName][Type]\n",
    "        match = re.match(r\"(.+?)\\[(.*?)\\]\\s+(increases|decreases|positively correlate with|negatively correlate with|causes)\\s+(.+?)\\[(.*?)\\]$\", line)\n",
    "        if match:\n",
    "            subject_name, subject_type, relation, object_name, object_type = match.groups()\n",
    "            results.append({\n",
    "                \"subject\": {\"name\": subject_name.strip(), \"type\": subject_type.strip()},\n",
    "                \"relationship\": relation.strip(),\n",
    "                \"object\": {\"name\": object_name.strip(), \"type\": object_type.strip()}\n",
    "            })\n",
    "    return results\n",
    "\n",
    "parsed = parse_relationships(relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Print results\n",
    "print(\"\\nExtracted relationships:\")\n",
    "if relationships:\n",
    "    for rel in relationships:\n",
    "        print(f\"- {rel}\")\n",
    "else:\n",
    "    print(\"No Relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbdb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create complete dictionaries for entities and relationships\n",
    "entities_dict = [\n",
    "    {\n",
    "        \"id\": entity.id,\n",
    "        \"text\": entity.text,\n",
    "        \"type\": entity.type.value,\n",
    "        \"aliases\": entity.aliases,\n",
    "        \"metadata\": entity.metadata\n",
    "    } \n",
    "    for entity in entities\n",
    "]\n",
    "\n",
    "relationships_dict = [\n",
    "    {\n",
    "        \"subject\": {\n",
    "            \"id\": rel.subject.id,\n",
    "            \"text\": rel.subject.text,\n",
    "            \"type\": rel.subject.type.value\n",
    "        },\n",
    "        \"object\": {\n",
    "            \"id\": rel.object.id,\n",
    "            \"text\": rel.object.text,\n",
    "            \"type\": rel.object.type.value\n",
    "        },\n",
    "        \"relation_type\": rel.relation_type.value,\n",
    "        \"polarity\": rel.polarity,\n",
    "        \"confidence\": rel.confidence,\n",
    "        \"evidence\": rel.evidence,\n",
    "        \"metadata\": rel.metadata\n",
    "    }\n",
    "    for rel in relationships\n",
    "] if relationships else []\n",
    "\n",
    "# Save entities to JSON file with complete information\n",
    "with open('entities.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(entities_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save relationships to JSON file with complete information\n",
    "with open('relationships.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(relationships_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Files saved successfully with complete data structures!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a98b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4mat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
